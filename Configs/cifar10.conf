%%%%%%%%%%%%%%%%%%%%%  Layers specification %%%%%%%%%%%%%%%%%%
 	net.layers{end+1}.properties = struct('type','input', 'sizeFm' ,[32 32],'numFm',3 );         %inputLayer
	net.layers{end+1}.properties = struct('type','conv', 'numFm',48 , 'Activation',@Relu, 'dActivation',@dRelu, 'kernel',5, 'stride',2);
	net.layers{end+1}.properties = struct('type','conv', 'numFm',48 , 'Activation',@Relu, 'dActivation',@dRelu, 'kernel',5, 'pad',1, 'dropOut',0.8);
	net.layers{end+1}.properties = struct('type','conv', 'numFm',96 , 'Activation',@Relu, 'dActivation',@dRelu, 'kernel',7, 'pad',2, 'dropOut',0.8);
	net.layers{end+1}.properties = struct('type','fc', 'numFm',128, 'Activation',@Relu, 'dActivation',@dRelu, 'dropOut',0.8);
	net.layers{end+1}.properties = struct('type','fc', 'numFm',10);
 	net.layers{end+1}.properties = struct('type','output','lossFunc',@CrossEnt,'costFunc',@CrossEnt_Cost);     %output Layer - classification

%%%%%%%%%%%%%%%%%%%%%  Hyper params - training %%%%%%%%%%%%%%%%%%

	net.hyperParam.trainLoopCount = 1000;		%on how many images to train before evaluating the network
	net.hyperParam.testImageNum   = 2000;   	% after each loop, on how many images to evaluate network performance
	net.hyperParam.ni_initial     = 0.0001;	        % ni to start training process
	net.hyperParam.ni_final       = 0.000001;	% final ni to stop the training process
	net.hyperParam.momentum       = 0.9;
    net.hyperParam.lambda         = 0.008; 		% L2 regularization factor
	net.hyperParam.flipImage      = 1;              % randomly flip the input hor/vert before passing to the network. Improves learning in some instances
	net.runInfoParam.verifyBP     = 0;       


